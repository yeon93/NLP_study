{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어 처리(Natural Language Processing)\n",
    "+ 자연어는 일상 생활에서 사용하는 언어\n",
    "+ 자연어 처리는 자연어의 의미를 분석 처리하는 일\n",
    "+ 응용 => 텍스트 분류, 감성 분석, 문서 요약, 번역, 질의 응답, 음성 인식, 챗봇"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'No pain no gain'\n",
    "\n",
    "'pain' in s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No', 'pain', 'no', 'gain']\n",
      "pain\n",
      "on\n"
     ]
    }
   ],
   "source": [
    "print(s.split())\n",
    "print(s.split()[1])\n",
    "print(s.split()[2][::-1])   #뒤에서부터 접근"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.index('n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 대소문자 통합\n",
    "+ 컴퓨터는 s와 S를 다른 글자로 인식\n",
    "+ lower(), upper() 를 통해 간단하게 통합 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCDEFGH\n",
      "abcdefgh\n"
     ]
    }
   ],
   "source": [
    "s = 'AbCdEFgH'\n",
    "\n",
    "print(s.upper())\n",
    "print(s.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규화(Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I visited United Kingdom from US on 2022-09-20\n",
      "I visited UK from United States on 2022-09-20\n",
      "I visited UK from US on 2021-09-20\n"
     ]
    }
   ],
   "source": [
    "s = 'I visited UK from US on 2022-09-20'\n",
    "\n",
    "print(s.replace('UK', 'United Kingdom'))\n",
    "print(s.replace('US', 'United States'))\n",
    "print(s.replace('22-', '21-'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규표현식\n",
    "+ 특정 문자들을 편리하게 지정, 추가/삭제 가능\n",
    "+ 데이터 전처리에서 많이 사용됨\n",
    "+ 파이썬의 re 패키지 사용 가능\n",
    "\n",
    "* 정규 표현식 문법\n",
    "  \n",
    "| 특수문자 | 설명 |\n",
    "| - | - |\n",
    "| `.` | 앞의 문자 1개를 표현 |\n",
    "| `?` | 문자 한개를 표현하나 존재할 수도, 존재하지 않을 수도 있음(0개 또는 1개) |\n",
    "| `*` | 앞의 문자가 0개 이상 |\n",
    "| `+` | 앞의 문자가 최소 1개 이상 |\n",
    "| `^` | 뒤의 문자로 문자열이 시작 |\n",
    "| `\\$` | 앞의 문자로 문자열이 끝남 |\n",
    "| `\\{n\\}` | `n`번만큼 반복 |\n",
    "| `\\{n1, n2\\}` | `n1` 이상, `n2` 이하만큼 반복, n2를 지정하지 않으면 `n1` 이상만 반복 |\n",
    "| `\\[ abc \\]` | 안에 문자들 중 한 개의 문자와 매치, a-z처럼 범위도 지정 가능 |\n",
    "| `\\[ ^a \\]` | 해당 문자를 제외하고 매치 |\n",
    "| `a\\|b` | `a` 또는 `b`를 나타냄 |\n",
    "\n",
    "* 정규 표현식에 자주 사용하는 역슬래시(\\\\)를 이용한 문자 규칙\n",
    "\n",
    "| 문자 | 설명 |\n",
    "| - | - |\n",
    "| `\\\\` | 역슬래시 자체를 의미 |\n",
    "| `\\d` | 모든 숫자를 의미, [0-9]와 동일 |\n",
    "| `\\D` | 숫자를 제외한 모든 문자를 의미, [^0-9]와 동일 |\n",
    "| `\\s` | 공백을 의미, [ \\t\\n\\r\\f\\v]와 동일|\n",
    "| `\\S` | 공백을 제외한 모든 문자를 의미, [^ \\t\\n\\r\\f\\v]와 동일 |\n",
    "| `\\w` | 문자와 숫자를 의미, [a-zA-Z0-9]와 동일 |\n",
    "| `\\W` | 문자와 숫자를 제외한 다른 문자를 의미, [^a-zA-Z0-9]와 동일 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='abc'>\n",
      "None\n",
      "None\n",
      "\n",
      "<re.Match object; span=(0, 2), match='ㅎㅎ'>\n",
      "None\n",
      "<re.Match object; span=(0, 2), match='안녕'>\n",
      "\n",
      "<re.Match object; span=(0, 3), match='abc'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#re.match(pattern, string)\n",
    "print(re.match('ab.', 'abc'))  #'ab.'  => [ab 뒤에 문자 하나]인 패턴\n",
    "print(re.match('ab.', 'ab'))\n",
    "print(re.match('ab.', 'c'))\n",
    "print()\n",
    "print(re.match('[ㄱ-ㅎ]+', 'ㅎㅎ안녕'))\n",
    "print(re.match('[ㄱ-ㅎ]+', '안녕ㅎㅎ'))\n",
    "print(re.match('[가-힣]+', '안녕ㅎㅎ'))\n",
    "print()\n",
    "\n",
    "#re.compile(pattern) 객체에 .match() 적용\n",
    "print(re.compile('ab.').match('abc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 1), match='a'>\n",
      "<re.Match object; span=(0, 2), match='ab'>\n",
      "None\n",
      "\n",
      "<re.Match object; span=(0, 2), match='ㄱㅏ'>\n",
      "None\n",
      "<re.Match object; span=(2, 4), match='ㄱㅏ'>\n"
     ]
    }
   ],
   "source": [
    "#re.search() => match와 달리 문자열 전체를 검사\n",
    "print(re.search('a', 'ab?'))    #'ab?' => [a 뒤에 b가 있거나/없거나]인 패턴\n",
    "print(re.search('ab', 'ab?'))\n",
    "print(re.search('abc', 'ab?'))\n",
    "print()\n",
    "print(re.search('[ㄱ-ㅎ|ㅏ-ㅣ]+', 'ㄱㅏ나다라'))\n",
    "print(re.match('[ㄱ-ㅎ|ㅏ-ㅣ]+', '안 ㄱㅏ'))\n",
    "print(re.search('[ㄱ-ㅎ|ㅏ-ㅣ]+', '안 ㄱㅏ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ab', 'cd', 'ef']\n",
      "['ab ', 'd ef']\n",
      "['a', 'bc ', 'd', 'e ', 'fgh ', 'i']\n"
     ]
    }
   ],
   "source": [
    "#split => 정규표현식에 해당하는 문자열을 기준으로 문자열 나누기\n",
    "print(re.compile(' ').split('ab cd ef'))\n",
    "print(re.compile('c').split('ab cd ef'))\n",
    "print(re.compile('[1-9]').split('a1bc 2d3e 4fgh 5i'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "abcdefg가나다123\n"
     ]
    }
   ],
   "source": [
    "#sub => 정규표현식와 일치하는 부분을 다른 문자열로 대체\n",
    "print(re.sub('[a-z]', 'abcdefg가나다123', '1'))\n",
    "print(re.sub('[^a-z]', 'abcdefg가나다123', '1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1', '2', '3', '4']\n",
      "['a', 'b', ' ', 'c', 'd', ' ', 'e', 'f', ' ', 'g', 'h', '!']\n",
      "['1', 'a', 'b', '2', 'c', 'd', '3', 'e', 'f', '4', 'g', 'h']\n",
      "[' ', ' ', ' ', '!']\n"
     ]
    }
   ],
   "source": [
    "#findall => compile한 정규표현식과 맞는 모든 문자(열)을 list로 반환\n",
    "print(re.findall('[\\d]', '1ab 2cd 3ef 4gh!'))  #\\d : 숫자\n",
    "print(re.findall('[\\D]', '1ab 2cd 3ef 4gh!'))  #\\D : 숫자가 아닌 것(문자, 특수문자)\n",
    "print(re.findall('[\\w]', '1ab 2cd 3ef 4gh!'))  #\\w : 문자, 숫자\n",
    "print(re.findall('[\\W]', '1ab 2cd 3ef 4gh!'))  #\\W : 문자, 숫자가 아닌 것(특수문자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<callable_iterator object at 0x00000208DABA4B00>\n",
      "<re.Match object; span=(0, 1), match='1'>\n",
      "<re.Match object; span=(4, 5), match='2'>\n",
      "<re.Match object; span=(8, 9), match='3'>\n",
      "<re.Match object; span=(12, 13), match='4'>\n"
     ]
    }
   ],
   "source": [
    "#finditer => compile한 정규표현식과 맞는 모든 문자(열)을 iterator 객체로 반환 => 생성된 객체를 하나씩 자동으로 가져올 수 있어 처리가 간편함\n",
    "iter1 = re.finditer('[\\d]', '1ab 2cd 3ef 4gh!')\n",
    "print(iter1)\n",
    "for i in iter1:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화(tokenization)\n",
    "+ 특수문자에 대한 처리\n",
    "    + 단어에 일반적으로 사용되는 알파벳, 숫자와 달리 별도의 처리 필요\n",
    "    + 일괄적으로 특수문자를 제거할 수도 있지만, 특수문자의 의미를 학습에 반영해야 할 수도 있음\n",
    "    + 따라서 일괄적으로 제거하기 보다는, 데이터의 특성을 파악하고 처리하는 것이 중요\n",
    "\n",
    "\n",
    "\n",
    "+ 특정 단어에 대한 토큰 분리 방법\n",
    "    + United States처럼 두 단어가 모여 새로운 의미를 가지는 경우, 사용자가 단어의 특성을 고려해 토큰을 분리해야 학습에 유리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'is', 'gold']\n",
      "['The world is a beautiful book.', 'But of little use to him who cannot read it.']\n"
     ]
    }
   ],
   "source": [
    "#.split() 사용\n",
    "s = 'Time is gold'\n",
    "p = 'The world is a beautiful book.\\nBut of little use to him who cannot read it.'\n",
    "\n",
    "s_tokens = [x for x in s.split(' ')]\n",
    "p_tokens = [x for x in p.split('\\n')]\n",
    "\n",
    "print(s_tokens)\n",
    "print(p_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Time', 'is', 'gold']\n",
      "['The world is a beautiful book.', 'But of little use to him who cannot read it.']\n"
     ]
    }
   ],
   "source": [
    "#nltk 패키지(natural language tool kit)의 tokenize 모듈의 word_tokenize(), sent_tokenize() 사용\n",
    "#!pip install nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "s_tokens = word_tokenize(s)\n",
    "p_tokens = sent_tokenize(p)\n",
    "\n",
    "print(s_tokens)\n",
    "print(p_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Where', 'there', 's', 'a', 'will', 'there', 's', 'a', 'way']\n",
      "['Where', \"there's\", 'a', 'will,', \"there's\", 'a', 'way']\n",
      "['Where', \"there's\", 'a', 'will,', \"there's\", 'a', 'way']\n"
     ]
    }
   ],
   "source": [
    "#정규표현식을 이용한 토큰화\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "s = 'Where there\\'s a will, there\\'s a way'\n",
    "\n",
    "tokenizer = RegexpTokenizer('[\\w]+')               #\\w => 문자 또는 숫자\n",
    "print(tokenizer.tokenize(s))\n",
    "\n",
    "tokenizer = RegexpTokenizer('[\\S]+')               #\\S => 공백문자가 아닌 것\n",
    "print(tokenizer.tokenize(s))\n",
    "\n",
    "tokenizer = RegexpTokenizer('[\\s]+', gaps=True)    #\\s => 공백문자, gaps => 그 패턴을 이용해 토큰 분리\n",
    "print(tokenizer.tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['where', \"there's\", 'a', 'will', \"there's\", 'a', 'way']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keras를 이용한 토큰화\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "text_to_word_sequence(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where there's a will, there's a way\n",
      "['Where', 'there', \"'s\", 'a', 'will', 'there', \"'s\", 'a', 'way']\n"
     ]
    }
   ],
   "source": [
    "#TextBlob을 이용한 토큰화\n",
    "from textblob import TextBlob\n",
    "\n",
    "print(TextBlob(s))\n",
    "print(TextBlob(s).words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### +) 기타 토크나이저\n",
    "+ WhiteSpaceTokenizer : 공백을 기준으로 토큰화\n",
    "+ WordPunktTokenizer : 텍스트를 알파벳, 숫자, 알파벳 외 문자 list로 토큰화\n",
    "+ MWETokenizer : Multi Word Expression의 약자로, 'United States'처럼 여러 단어로 이루어진 특정 그룹을 한 개체로 취급\n",
    "+ TweetTokenizer : 트위터에서 사용되는 문장의 토큰화를 위해 만들어졌으며, 문장 속 감성/감정을 다룸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-gram(2) with nltk => [('There', 'is'), ('is', 'no'), ('no', 'royal'), ('royal', 'road'), ('road', 'to'), ('to', 'learning')]\n",
      "n-gram(3) with nltk => [('There', 'is', 'no'), ('is', 'no', 'royal'), ('no', 'royal', 'road'), ('royal', 'road', 'to'), ('road', 'to', 'learning')]\n",
      "n_gram(2) with textblob => [WordList(['There', 'is']), WordList(['is', 'no']), WordList(['no', 'royal']), WordList(['royal', 'road']), WordList(['road', 'to']), WordList(['to', 'learning'])]\n"
     ]
    }
   ],
   "source": [
    "#n-gram 추출 : n개의 어절/음절을 연쇄적으로 분류해 빈도를 분석\n",
    "from nltk import ngrams\n",
    "from textblob import TextBlob\n",
    "\n",
    "s = 'There is no royal road to learning'\n",
    "\n",
    "bigram = list(ngrams(s.split(), 2))\n",
    "trigram = list(ngrams(s.split(), 3))\n",
    "blob_bigram = TextBlob(s).ngrams(n=2)\n",
    "\n",
    "print('n-gram(2) with nltk =>', bigram)\n",
    "print('n-gram(3) with nltk =>', trigram)\n",
    "print('n_gram(2) with textblob =>', blob_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Think', 'like', 'man', 'of', 'action', 'and', 'act', 'like', 'man', 'of', 'thought', '.']\n",
      "[('Think', 'VBP'), ('like', 'IN'), ('man', 'NN'), ('of', 'IN'), ('action', 'NN'), ('and', 'CC'), ('act', 'NN'), ('like', 'IN'), ('man', 'NN'), ('of', 'IN'), ('thought', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#PoS(Parts of Speech) 태깅 : 문장 내에서 각 단어에 해당하는 품사를 태깅\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "from nltk import word_tokenize\n",
    "\n",
    "s = 'Think like man of action and act like man of thought.'\n",
    "words = word_tokenize(s)   #띄어쓰기 기준 토큰화\n",
    "print(words)\n",
    "\n",
    "print(nltk.pos_tag(words)) #각 토큰의 품사 태깅"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 불용어 제거\n",
    "+ 영어의 전치사(on, in), 한국어의 전치사(을/를) 등은 분석에 필요하지 않은 경우가 많음\n",
    "+ 길이가 짧은 단어, 빈도 수가 적은 단어들도 분석에 큰 영향을 주지 않음\n",
    "+ 일반적으로 사용되는 도구들은 해당 단어들을 제거해주지만, 완벽하게 제거되지는 않음  \n",
    "=> 사용자가 불용어 사전을 만들어 해당 단어들을 제거하는 것이 좋음  \n",
    "=> 도구들이 걸러주지 않는 전치사, 조사 등을 불용어 사전으로 만들면 됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['singer', 'stage']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#직접 불용어 사전 만들기\n",
    "stop_words = ['on', 'in', 'the']\n",
    "s = 'singer on the stage'.split()\n",
    "\n",
    "nouns = []\n",
    "for noun in s:\n",
    "    if noun not in stop_words:\n",
    "        nouns.append(noun)\n",
    "        \n",
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
     ]
    }
   ],
   "source": [
    "#nltk의 불용어 리스트 사용\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "print(len(stop_words))\n",
    "print(stop_words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If', 'walk', 'today', ',', 'run', 'tomorrow']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'If you do not walk today, you will have to run tomorrow'\n",
    "words = word_tokenize(s)\n",
    "\n",
    "no_stop_words = []\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        no_stop_words.append(w)\n",
    "        \n",
    "no_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 오타 교정\n",
    "+ 텍스트에 오탈자가 존재하는 경우, 사람은 적절한 추정을 통해 이해하는 데에 문제가 없지만, 컴퓨터는 오탈자를 그대로 받아들여 처리함\n",
    "+ 철자 교정 알고리즘은 이미 개발되어, 워드 프로세서 등 다양한 서비스에서 많이 적용됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "people\n",
      "apple\n",
      "language\n"
     ]
    }
   ],
   "source": [
    "#!pip install autocorrect\n",
    "from autocorrect import Speller\n",
    "\n",
    "speller = Speller('en')   #영어\n",
    "\n",
    "print(speller('peoplle'))\n",
    "print(speller('aplle'))\n",
    "print(speller('langage'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Earlly', 'biird', 'catchess', 'the', 'womm', '.']\n",
      "['Early', 'bird', 'catches', 'the', 'worm', '.']\n"
     ]
    }
   ],
   "source": [
    "s = word_tokenize('Earlly biird catchess the womm.')\n",
    "ss = [speller(i) for i in s]\n",
    "\n",
    "print(s)\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 언어의 단수화, 복수화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apples', 'bananas', 'oranges', 'grapes', 'teeth']\n",
      "['apple', 'banana', 'orange', 'grape', 'tooth']\n"
     ]
    }
   ],
   "source": [
    "fruit = 'apple banana orange grape tooth'\n",
    "fruits = 'apples bananas oranges grapes teeth'\n",
    "\n",
    "tb = TextBlob(fruit)\n",
    "tbs = TextBlob(fruits)\n",
    "\n",
    "print(tb.words.pluralize())      #단수 => 복수\n",
    "print(tbs.words.singularize())   #복수 => 단수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 어간(stemming) 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "applic\n",
      "begin\n",
      "catch\n",
      "educ\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "print(stemmer.stem('application'))\n",
    "print(stemmer.stem('beginning'))\n",
    "print(stemmer.stem('catches'))\n",
    "print(stemmer.stem('education'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 표제어(lemmatization) 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application\n",
      "beginning\n",
      "catch\n",
      "education\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('application'))\n",
    "print(lemmatizer.lemmatize('beginning'))\n",
    "print(lemmatizer.lemmatize('catches'))\n",
    "print(lemmatizer.lemmatize('education'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 개체명 인식(named entity recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rome', 'NNP'), ('was', 'VBD'), ('not', 'RB'), ('built', 'VBN'), ('in', 'IN'), ('a', 'DT'), ('day', 'NN')]\n",
      "(S (NE Rome/NNP) was/VBD not/RB built/VBN in/IN a/DT day/NN)\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "\n",
    "s = 'Rome was not built in a day'\n",
    "\n",
    "tags = nltk.pos_tag(word_tokenize(s))\n",
    "print(tags)\n",
    "\n",
    "entities = nltk.ne_chunk(tags, binary=True)\n",
    "print(entities)   #NE(Named Entity) 태그가 붙은 'Rome'이 개체명임을 알 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 중의성(lexical ambiguity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('saw.v.01')\n",
      "Synset('squash_racket.n.01')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "\n",
    "s = 'I saw bats.'   #나는, 봤다/톱질했다, 박쥐들/야구배트들\n",
    "\n",
    "print(lesk(word_tokenize(s), 'saw'))    #verb(동사)\n",
    "print(lesk(word_tokenize(s), 'bats'))   #noun(명사)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어 토큰화\n",
    "+ 한국어는 띄어쓰기가 완벽하지 않아도 의미가 전달되는 경우가 많고, 띄어쓰기가 지켜지지 않았을 가능성 존재\n",
    "+ 띄어쓰기가 지켜지지 않으면 정상적인 토큰 분리가 어려움\n",
    "+ 한국어에는 형태소라는 개념이 있어 추가로 고려해줘야 함\n",
    "+ '그는', '그가' 같은 단어들은 의미는 'he'로 같지만 텍스트 처리에서 다르게 받아들일 수 있어 처리 필요\n",
    "\n",
    "### 단어 토큰화\n",
    "+ 띄어쓰기 기준으로 단어를 분리해도 조사, 접속사 등이 남아있어 분석에 어려움이 있음\n",
    "+ konlpy, MeCab 등 라이브러리를 이용해 조사, 접속사를 분리/제거 가능\n",
    "+ 그러나 라이브러리를 사용해도 한국어의 전치표현 등으로 인해 제대로 토큰화가 어려움 => 사용자가 별도 처리 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화만 => ['아버지', '가', '방', '에', '들어가', '신다', '.']\n",
      "형태소분석 => [('아버지', 'NNG'), ('가', 'JKS'), ('방', 'NNG'), ('에', 'JKB'), ('들어가', 'VV'), ('신다', 'EP+EF'), ('.', 'SF')]\n",
      "명사만 => ['아버지', '방']\n"
     ]
    }
   ],
   "source": [
    "#한국어 자연어처리 konlpy, 형태소 분석기 MeCab\n",
    "#!pip install konlpy\n",
    "#windows에서 MeCab 설치하는 방법 : https://uwgdqo.tistory.com/363\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab(dicpath=r\"C:/mecab/mecab-ko-dic\")\n",
    "\n",
    "print('토큰화만 =>', mecab.morphs('아버지가방에들어가신다.'))\n",
    "print('형태소분석 =>', mecab.pos('아버지가방에들어가신다.'))\n",
    "print('명사만 =>', mecab.nouns('아버지가방에들어가신다.'))    #조사, 접속사 등 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['진짜? 내일 어디 가지...', '이렇게 문장이 여러개 연결되어 있어도 나눠질까?', '밥은 먹었어?', '나는...']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install kss\n",
    "import kss\n",
    "\n",
    "s = '진짜? 내일 어디 가지... 이렇게 문장이 여러개 연결되어 있어도 나눠질까? 밥은 먹었어? 나는...'\n",
    "\n",
    "kss.split_sentences(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['안녕하세요', '저는', '자연어', '처리', '에', '관심이', '있는', '학생입니다']\n",
      "['안녕하세요 ', ' 저는 자연어 처리(NLP)에 관심이 있는', ' 학생입니다.']\n"
     ]
    }
   ],
   "source": [
    "#정규표현식을 이용한 토큰화\n",
    "s = '안녕하세요 ㅋㅋ 저는 자연어 처리(NLP)에 관심이 있는ㄴ 학생입니다.'   #비문으로 테스트\n",
    "\n",
    "tokenizer = RegexpTokenizer('[가-힣]+')\n",
    "tokens = tokenizer.tokenize(s)\n",
    "print(tokens)\n",
    "\n",
    "tokenizer = RegexpTokenizer('[ㄱ-ㅎ]+', gaps=True)\n",
    "tokens = tokenizer.tokenize(s)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['성공의', '비결은', '단', '한가지', '잘할', '수', '있는', '일에', '광적으로', '집중하는', '것이다']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keras를 이용한 토큰화\n",
    "s = '성공의 비결은 단 한가지, 잘할 수 있는 일에 광적으로 집중하는 것이다.'\n",
    "\n",
    "text_to_word_sequence(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['성공의', '비결은', '단', '한가지', '잘할', '수', '있는', '일에', '광적으로', '집중하는', '것이다'])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TextBlob을 이용한 토큰화\n",
    "\n",
    "TextBlob(s).words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW(Bag of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 2 2 2 1 1]]\n",
      "{'think': 6, 'like': 3, 'man': 4, 'of': 5, 'action': 1, 'and': 2, 'act': 0, 'thought': 7}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "s = ['Think like a man of action and act like a man of thought.']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "bow = cv.fit_transform(s)\n",
    "\n",
    "print(bow.toarray())    #인덱스별 등장횟수\n",
    "print(cv.vocabulary_)   #단어:인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 2 2 1 1]]\n",
      "{'think': 4, 'like': 2, 'man': 3, 'action': 1, 'act': 0, 'thought': 5}\n"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer(stop_words='english')   #불용어 추가\n",
    "bow = cv.fit_transform(s)\n",
    "\n",
    "print(bow.toarray())    #인덱스별 등장횟수\n",
    "print(cv.vocabulary_)   #단어:인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1 1 1 1 1 1 1 1]]\n",
      "{'평생': 8, '것처럼': 0, '꿈을': 3, '꾸어라': 2, '그리고': 1, '내일': 4, '죽을': 7, '오늘을': 6, '살아라': 5}\n"
     ]
    }
   ],
   "source": [
    "s = ['평생 살 것처럼 꿈을 꾸어라. 그리고 내일 죽을 것처럼 오늘을 살아라.']\n",
    "\n",
    "cv = CountVectorizer()\n",
    "bow = cv.fit_transform(s)\n",
    "\n",
    "print(bow.toarray())    #인덱스별 등장횟수\n",
    "print(cv.vocabulary_)   #단어:인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1]\n",
      "{'평생': 0, '살': 1, '것': 2, '처럼': 3, '꿈': 4, '을': 5, '꾸': 6, '어라': 7, '그리고': 8, '내일': 9, '죽': 10, '오늘': 11, '아라': 12}\n"
     ]
    }
   ],
   "source": [
    "s = '평생 살 것처럼 꿈을 꾸어라. 그리고 내일 죽을 것처럼 오늘을 살아라.'\n",
    "\n",
    "mecab = Mecab(dicpath=r\"C:/mecab/mecab-ko-dic\")\n",
    "tokens = mecab.morphs(re.sub('(\\.)', '', s))\n",
    "\n",
    "vocab, bow = {}, []\n",
    "\n",
    "for token in tokens:\n",
    "    if token not in vocab.keys():\n",
    "        vocab[token] = len(vocab)\n",
    "        bow.insert(len(vocab)-1, 1)\n",
    "    else:\n",
    "        index = vocab.get(token)\n",
    "        bow[index] += 1\n",
    "        \n",
    "print(bow)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DTM(Document Term Matrix, 문서 단어 행렬)\n",
    "+ 문서에 등장하는 여러 단어들의 빈도를 행렬로 표현\n",
    "+ 즉, 각 문서에 대한 BoW를 하나의 행렬로 표현한 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 2 2 0 1 1 0 0]\n",
      " [0 0 0 0 0 2 1 0 0 2 1]\n",
      " [0 0 1 1 0 0 0 0 0 0 0]]\n",
      "[('act', 0), ('action', 1), ('death', 2), ('liberty', 3), ('like', 4), ('man', 5), ('success', 6), ('think', 7), ('thought', 8), ('try', 9), ('value', 10)]\n"
     ]
    }
   ],
   "source": [
    "s = ['Think like a man of action and act like man of thought.',\n",
    "     'Try not to become a man of success but rather try to become a man of value.',\n",
    "     'Give me liberty, or give me death.']\n",
    "\n",
    "cv = CountVectorizer(stop_words='english')   \n",
    "bow = cv.fit_transform(s)\n",
    "\n",
    "print(bow.toarray())  \n",
    "print(sorted(cv.vocabulary_.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>death</th>\n",
       "      <th>liberty</th>\n",
       "      <th>like</th>\n",
       "      <th>man</th>\n",
       "      <th>success</th>\n",
       "      <th>think</th>\n",
       "      <th>thought</th>\n",
       "      <th>try</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   act  action  death  liberty  like  man  success  think  thought  try  value\n",
       "0    1       1      0        0     2    2        0      1        1    0      0\n",
       "1    0       0      0        0     0    2        1      0        0    2      1\n",
       "2    0       0      1        1     0    0        0      0        0    0      0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = []\n",
    "for k, v in sorted(cv.vocabulary_.items(), key=lambda item:item[1]):\n",
    "    columns.append(k)\n",
    "    \n",
    "pd.DataFrame(bow.toarray(), columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF(Term Frequency-Inverse Document Frequency, 어휘빈도-문서역빈도 분석)\n",
    "+ 단순히 빈도수가 높은 단어를 핵심어로 보는 것이 아니라, 해당 단어가 특정 문서에만 집중적으로 등장할 때를 주목"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.311383   0.311383   0.         0.         0.62276601 0.4736296\n",
      "  0.         0.311383   0.311383   0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.52753275\n",
      "  0.34682109 0.         0.         0.69364217 0.34682109]\n",
      " [0.         0.         0.70710678 0.70710678 0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n",
      "[('act', 0), ('action', 1), ('death', 2), ('liberty', 3), ('like', 4), ('man', 5), ('success', 6), ('think', 7), ('thought', 8), ('try', 9), ('value', 10)]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english').fit(s)\n",
    "\n",
    "print(tfidf.transform(s).toarray())\n",
    "print(sorted(tfidf.vocabulary_.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>action</th>\n",
       "      <th>death</th>\n",
       "      <th>liberty</th>\n",
       "      <th>like</th>\n",
       "      <th>man</th>\n",
       "      <th>success</th>\n",
       "      <th>think</th>\n",
       "      <th>thought</th>\n",
       "      <th>try</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.311383</td>\n",
       "      <td>0.311383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.622766</td>\n",
       "      <td>0.473630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.311383</td>\n",
       "      <td>0.311383</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.527533</td>\n",
       "      <td>0.346821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693642</td>\n",
       "      <td>0.346821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.707107</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        act    action     death   liberty      like       man   success  \\\n",
       "0  0.311383  0.311383  0.000000  0.000000  0.622766  0.473630  0.000000   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.527533  0.346821   \n",
       "2  0.000000  0.000000  0.707107  0.707107  0.000000  0.000000  0.000000   \n",
       "\n",
       "      think   thought       try     value  \n",
       "0  0.311383  0.311383  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.693642  0.346821  \n",
       "2  0.000000  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = []\n",
    "for k, v in sorted(tfidf.vocabulary_.items(), key=lambda item:item[1]):\n",
    "    columns.append(k)\n",
    "    \n",
    "pd.DataFrame(tfidf.transform(s).toarray(), columns=columns)  #값이 클수록 해당 문서에서 중요한 단어"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce0eed95050303a93ceb779e08443ad560cb5ca5b89bcc7b5d112f6b77cc05f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
