{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 문서 분류(Document Classification)\n",
    "\n",
    "#### 데이터 준비\n",
    "+ 문서 분류에 필요한 데이터는 `scikit-learn`이 제공하는 20개의 주제를 가지는 뉴스그룹 데이터를 사용\n",
    "+ 텍스트는 `CounterVectorizer`를 거쳐 DTM 행렬로 변환\n",
    "+ DTM 행렬은 문서에 등장하는 단어들을 빈도 수 별로 표현한 행렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "(9051, 130107) (9051,) (2263, 130107) (2263,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "news = fetch_20newsgroups()\n",
    "\n",
    "x, y = news.data, news.target\n",
    "\n",
    "print(x[0])\n",
    "\n",
    "cv = CountVectorizer()\n",
    "x = cv.fit_transform(x)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n",
    "\n",
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 56979)\t2\n",
      "  (0, 50527)\t5\n",
      "  (0, 124031)\t1\n",
      "  (0, 85354)\t2\n",
      "  (0, 111322)\t1\n",
      "  (0, 123984)\t1\n",
      "  (0, 68532)\t2\n",
      "  (0, 114731)\t3\n",
      "  (0, 87620)\t1\n",
      "  (0, 95162)\t1\n",
      "  (0, 64095)\t1\n",
      "  (0, 90379)\t1\n",
      "  (0, 118983)\t1\n",
      "  (0, 89362)\t1\n",
      "  (0, 76032)\t1\n",
      "  (0, 123292)\t1\n",
      "  (0, 28615)\t1\n",
      "  (0, 90774)\t1\n",
      "  (0, 80638)\t1\n",
      "  (0, 114455)\t7\n",
      "  (0, 68766)\t2\n",
      "  (0, 115475)\t3\n",
      "  (0, 66608)\t4\n",
      "  (0, 26073)\t1\n",
      "  (0, 73201)\t2\n",
      "  :\t:\n",
      "  (0, 52959)\t1\n",
      "  (0, 105418)\t6\n",
      "  (0, 53166)\t1\n",
      "  (0, 55420)\t1\n",
      "  (0, 41049)\t2\n",
      "  (0, 9787)\t2\n",
      "  (0, 100425)\t1\n",
      "  (0, 64171)\t1\n",
      "  (0, 118017)\t4\n",
      "  (0, 66887)\t1\n",
      "  (0, 116393)\t1\n",
      "  (0, 110338)\t6\n",
      "  (0, 39816)\t1\n",
      "  (0, 112222)\t3\n",
      "  (0, 91972)\t2\n",
      "  (0, 27638)\t1\n",
      "  (0, 114419)\t4\n",
      "  (0, 69914)\t1\n",
      "  (0, 50730)\t3\n",
      "  (0, 62766)\t1\n",
      "  (0, 62005)\t2\n",
      "  (0, 48763)\t1\n",
      "  (0, 63071)\t1\n",
      "  (0, 80860)\t1\n",
      "  (0, 2634)\t1\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "#(x번째 문서, 단어의 index)  등장빈도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-learn을 이용한 문서 분류\n",
    "#### 1. 로지스틱 회귀\n",
    "+ 선형회귀모델에 sigmoid 함수 적용 => 이진분류\n",
    "+ 다중분류에는 적합하지 않음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\yeonok\\.conda\\envs\\nlp\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8683163941670349"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train, y_train)\n",
    "\n",
    "pred = lr.predict(x_test)\n",
    "\n",
    "acc = accuracy_score(pred, y_test)\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. SVC(서포트 벡터 머신)\n",
    "+ 회귀, 분류, 이상치 탐지 등에 사용되는 지도학습 방법\n",
    "+ 서포트 벡터 : 클래스 사이 경계에 위치한 데이터 포인트\n",
    "+ 각 서포트 벡터가 결정 경계를 구분하는 데 얼마나 중요한지를 학습하며, 각 서포트 벡터 사이 마진이 커지는 방향으로 학습\n",
    "+ 서포트 벡터까지의 거리와 서포트 벡터의 중요도를 기반으로 예측 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82545293857711"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "SVM = svm.SVC(kernel='linear')\n",
    "SVM.fit(x_train, y_train)\n",
    "\n",
    "pred = SVM.predict(x_test)\n",
    "acc = accuracy_score(pred, y_test)\n",
    "\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Naive Bayes Classification\n",
    "+ 베이즈 정리를 이용한 확률적 분류 알고리즘\n",
    "+ 모든 특성들이 독립임을 가정하며, 입력 특성에 따라 3개의 분류기 존재\n",
    "    + 입력데이터가 연속형일 때 => GaussianNB \n",
    "    + 입력데이터가 범주형일 때 => BernoulliNB, MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6743261157755193, 0.827220503756076)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "\n",
    "bnb = BernoulliNB()\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "bnb.fit(x_train, y_train)\n",
    "mnb.fit(x_train, y_train)\n",
    "\n",
    "b_pred = bnb.predict(x_test)\n",
    "m_pred = mnb.predict(x_test)\n",
    "\n",
    "b_acc = accuracy_score(b_pred, y_test)\n",
    "m_acc = accuracy_score(m_pred, y_test)\n",
    "\n",
    "b_acc, m_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. TF-IDF를 이용한 accuracy 향상"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8457799381352188"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf = TfidfTransformer()\n",
    "transformed_x_train = tfidf.fit_transform(x_train)\n",
    "transformed_x_test = tfidf.fit_transform(x_test)\n",
    "\n",
    "mnb.fit(transformed_x_train, y_train)\n",
    "pred = mnb.predict(transformed_x_test)\n",
    "acc = accuracy_score(pred, y_test)\n",
    "acc   #약 2% 향상됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Decision Tree\n",
    "+ 분류와 회귀에 사용되는 지도학습 방법\n",
    "+ 데이터 특성으로부터 추론된 if-then-else 결정 규칙을 통해 예측\n",
    "+ 트리의 깊이가 깊을수록 복잡한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6420680512593901"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(x_train, y_train)\n",
    "\n",
    "pred = dt.predict(x_test)\n",
    "acc = accuracy_score(pred, y_test)\n",
    "\n",
    "acc   #index 데이터만으로 규칙을 찾기 어려워 결정트리 성능 낮게 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. XGBoost\n",
    "+ 트리 기반의 앙상블 기법\n",
    "+ 분류에서 다른 알고리즘보다 좋은 예측 성능을 보여줌\n",
    "+ GBM의 느린 수행시간과 과적합 규제 부재 등의 문제를 해결\n",
    "+ 병렬 CPU 환경에서 빠르게 학습 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7123287671232876"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install xgboost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=30, learning_rate=0.05, max_depth=3)\n",
    "xgb.fit(x_train, y_train)\n",
    "\n",
    "pred = xgb.predict(x_test)\n",
    "acc = accuracy_score(pred, y_test)\n",
    "\n",
    "acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 교차검증\n",
    "+ 일반 검증은 학습데이터가 검증데이터로 사용되지 않음\n",
    "+ 교차검증은 전체데이터를 n개의 집합으로 나누어 평가하므로 학습데이터도 검증데이터로 사용됨\n",
    "+ 교차검증을 통해 일반검증보다 모델의 일반화가 잘 되는지 평가 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.83870968, 0.83826779, 0.82368537, 0.83031374, 0.83642794]),\n",
       " 0.833480903927519)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cv_scores = cross_val_score(mnb, x, y,                  #위에서 학습된 MultinomialNB 모델을 x, y 데이터를 이용해\n",
    "                            cv=5, scoring='accuracy')   #5회 교차검증, 예측 성능 평가지표는 정확도\n",
    "cv_scores, cv_scores.mean()                             #그 결과 평균 83%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정밀도, 재현율\n",
    "+ 정밀도(precision)\n",
    "    + 양성 클래스로 예측한 샘플이 실제 양성 클래스일 확률\n",
    "    + ex) 코로나 양성 판정 받은 100명 중 실제 코로나에 걸린 사람이 90명 => 정밀도 0.9\n",
    "+ 재현율(recall)\n",
    "    + 실제 양성 클래스인 샘플 중에서 모델이 양성 클래스로 예측한 샘플의 비율\n",
    "    + ex) 실제 코로나에 걸린 사람 100명 중 코로나 양성 판정 받은 사람이 99명 => 재현율 0.99\n",
    "+ F1-score\n",
    "    + 정밀도와 재현율의 가중조화평균\n",
    "\n",
    "\n",
    "+ 다중 클래스 분류 문제에서 정밀도, 재현율을 계산할 때 클래스간 지표를 어떻게 합칠지 지정 필요\n",
    "    + None : 클래스간 지표를 합치지 않고 그대로 출력\n",
    "    + micro : 정밀도와 재현율이 같음 => F1-score도 같은 값\n",
    "    + macro : 클래스간 지표를 단순 평균한 값\n",
    "    + weighted : 클래스간 지표를 가중 평균한 값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average='micro'일 때\n",
      "=> 0.7123287671232876 0.7123287671232876 0.7123287671232877\n",
      "average='macro'일 때\n",
      "=> 0.7030340507754758 0.7449670935600474 0.7140522110447666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "precision = precision_score(pred, y_test, average='micro')\n",
    "recall = recall_score(pred, y_test, average='micro')\n",
    "f1 = f1_score(pred, y_test, average='micro')\n",
    "\n",
    "print(\"average='micro'일 때\\n=>\", precision, recall, f1)\n",
    "\n",
    "precision = precision_score(pred, y_test, average='macro')\n",
    "recall = recall_score(pred, y_test, average='macro')\n",
    "f1 = f1_score(pred, y_test, average='macro')\n",
    "\n",
    "print(\"average='macro'일 때\\n=>\", precision, recall, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch를 이용한 파라미터 최적화\n",
    "+ estimator : 사용할 모델 객체\n",
    "+ param_grid : 지정 파라미터 리스트로 구성된 딕셔너리\n",
    "+ scoring : 최적화하고자하는 성능 지표\n",
    "+ cv : 교차검증 분할 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8897820965842167\n",
      "{'alpha': 0.001}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gs = GridSearchCV(estimator=mnb, \n",
    "                  param_grid={'alpha':[0.001, 0.01, 0.1, 1.],},\n",
    "                  scoring='accuracy',\n",
    "                  cv=10)\n",
    "gs.fit(x, y)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce0eed95050303a93ceb779e08443ad560cb5ca5b89bcc7b5d112f6b77cc05f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
